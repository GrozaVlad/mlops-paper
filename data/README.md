# Data Directory Structure

This directory contains all data used in the MLOps Drug Repurposing project, organized for clear separation and version control with DVC.

## Directory Structure

```
data/
├── raw/                    # Original, immutable datasets
│   ├── biosnap/           # BIOSNAP drug-target interaction data
│   ├── bindingdb/         # BindingDB drug-target binding data
│   └── external/          # Other external datasets
├── processed/             # Cleaned and preprocessed data
│   ├── train/            # Training datasets
│   ├── validation/       # Validation datasets
│   ├── test/             # Test datasets
│   └── features/         # Extracted molecular features
└── external/              # External reference data
    ├── drug_metadata/    # Drug compound metadata
    ├── target_metadata/  # Target protein metadata
    └── annotations/      # Biological annotations
```

## Data Sources

### BIOSNAP Dataset
- **Source**: Stanford BIOSNAP (Biological Network datasets)
- **Description**: Drug-target interaction networks
- **Format**: CSV files with drug-target pairs and interaction labels
- **Size**: ~2M drug-target interactions
- **License**: Academic use

### BindingDB Dataset  
- **Source**: BindingDB (Binding Database)
- **Description**: Quantitative binding affinity data
- **Format**: TSV files with binding measurements
- **Size**: ~2.7M binding measurements
- **License**: Open access

## Data Management with DVC

### Tracking Raw Data
```bash
# Add raw datasets to DVC tracking
dvc add data/raw/biosnap/
dvc add data/raw/bindingdb/

# Commit DVC files to git
git add data/raw/biosnap.dvc data/raw/bindingdb.dvc .gitignore
git commit -m "Add raw datasets to DVC tracking"

# Push data to S3 remote
dvc push
```

### Data Pipeline Stages
1. **Data Download**: Automated scripts to download latest datasets
2. **Data Validation**: Quality checks and schema validation
3. **Data Preprocessing**: Cleaning, filtering, and feature extraction
4. **Data Splitting**: Train/validation/test splits with stratification

### Data Versioning
- Raw data: Immutable, versioned with DVC
- Processed data: Generated by reproducible pipelines
- Features: Cached and versioned for consistency
- Metadata: Tracked alongside datasets

## Usage Guidelines

### Adding New Data
1. Place raw data in appropriate `data/raw/` subdirectory
2. Add data quality checks and documentation
3. Use DVC to track large files: `dvc add data/raw/new_dataset/`
4. Commit DVC files to git: `git add data/raw/new_dataset.dvc`
5. Push data to remote: `dvc push`

### Accessing Data
```python
import pandas as pd
from pathlib import Path

# Load processed training data
train_data = pd.read_csv("data/processed/train/drug_target_interactions.csv")

# Load molecular features
features = pd.read_parquet("data/processed/features/molecular_fingerprints.parquet")
```

### Data Validation
- All datasets include schema validation
- Quality checks are automated in DVC pipelines
- Data drift detection monitors for changes

## Security and Privacy
- No personal or sensitive data included
- All datasets are publicly available or properly licensed
- Data access follows institutional guidelines
- S3 storage uses encryption at rest

## Reproducibility
- All data transformations are scripted and versioned
- DVC ensures exact data versions for experiments
- Pipeline parameters are tracked in `dvc.yaml`
- Random seeds are fixed for reproducible splits